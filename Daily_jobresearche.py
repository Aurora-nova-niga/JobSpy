from __future__ import annotations

import os
import json
import time
import re
import random
import traceback
from datetime import datetime
from typing import Any, Dict, List, Optional
from dotenv import load_dotenv
from pathlib import Path
import pandas as pd
import requests
import msal
from openai import OpenAI
from jobspy import scrape_jobs


# -----------------------------
# 0) Config
# -----------------------------

load_dotenv()

OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
ASSISTANT_ID = os.environ["OPENAI_ASSISTANT_ID"]  # asst_...
NOTION_TOKEN = os.environ["NOTION_TOKEN"]
NOTION_DB_ID = os.environ["NOTION_DB_ID"]

client = OpenAI(api_key=OPENAI_API_KEY)

CV_FILE_ID = os.environ["OPENAI_CV_FILE_ID"]
CV_VECTOR_STORE_ID = os.environ["OPENAI_CV_VECTOR_STORE_ID"]


# JobSpyæŠ“å–å‚æ•°ï¼ˆä½ æŒ‰éœ€æ”¹ï¼‰
JOBSPY_CONFIG = {
    "sites": ["linkedin", "indeed"],
    "search_term": (
        '"supply chain engineer" OR '
        '"logistics engineer" OR '
        '"supply chain analyst" OR '
        '"ingÃ©nieur supply chain" OR '
        '"ingÃ©nieur logistique" OR '
        '"coordinateur supply chain" OR '
        '"coordinateur logistique" OR '
        '"consultant supply chain" OR '
        '"chef de projet supply chain" OR '
        '"chef de projet logistique" OR '
        '"ingÃ©nieur projet"'
    ),
    "location": "Strasbourg, France",
    "hours_old": 72,
    "results_wanted": 10,

    "linkedin_fetch_description": True,
    "description_format": "markdown",
    "verbose": 1,
}

# AI åˆ†æé…ç½®
AI_CONFIG = {
    "batch_size": 1,  # 1 = æ¯æ¬¡å¤„ç† 1 ä¸ªå²—ä½ï¼ˆæœ€å¯é ï¼‰
                      # 0 = ä¸åˆ†æ‰¹ï¼Œä¸€æ¬¡æ€§åˆ†ææ‰€æœ‰å²—ä½
                      # N = æ¯æ‰¹ N ä¸ªå²—ä½
                      # âš ï¸ ç”±äº Assistant è¾“å‡º token é™åˆ¶ï¼Œæ¨èè®¾ç½®ä¸º 1
    "max_retries": 3,  # API è°ƒç”¨å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé’ˆå¯¹ rate limit ç­‰ä¸´æ—¶é”™è¯¯ï¼‰
}

# ğŸ’¡ ä½¿ç”¨å»ºè®®ï¼š
# 1. å½“å‰è®¾ç½® batch_size = 1 æ˜¯å› ä¸ºï¼š
#    - AI Assistant å¯èƒ½æœ‰è¾“å‡º token é™åˆ¶ï¼ˆä¾‹å¦‚ max_tokens è®¾ç½®å¤ªå°ï¼‰
#    - å³ä½¿å‘é€ 3 ä¸ªå²—ä½ï¼ŒAI ä¹Ÿåªè¿”å› 1 ä¸ªåˆ†æ
#    - æ¯æ¬¡å¤„ç† 1 ä¸ªå²—ä½æœ€å¯é ï¼Œè™½ç„¶æ…¢ä½†ç¡®ä¿æˆåŠŸ
#
# 2. å¦‚ä½•æé«˜æ•ˆç‡ï¼ˆåœ¨ç¡®ä¿ç¨³å®šåï¼‰ï¼š
#    a) åœ¨ OpenAI å¹³å°æ£€æŸ¥ Assistant è®¾ç½®ï¼š
#       - æ‰¾åˆ°ä½ çš„ Assistant (asst_...)
#       - æ£€æŸ¥ "Response format" å’Œ token é™åˆ¶
#       - å¦‚æœæœ‰ max_tokens é™åˆ¶ï¼Œæé«˜åˆ° 4096 æˆ–æ›´é«˜
#    b) ä¿®æ”¹ Assistant çš„ instructionsï¼Œå¼ºè°ƒ"å¿…é¡»è¾“å‡ºæ•°ç»„"
#    c) ç„¶åå¯ä»¥å°è¯•å¢åŠ  batch_size åˆ° 2 æˆ– 3
#
# 3. å¦‚æœé¢‘ç¹é‡åˆ° "rate_limit_exceeded" é”™è¯¯ï¼š
#    - batch_size = 1 æ—¶æ¯æ¬¡è¯·æ±‚é—´éš” 2 ç§’ï¼Œé€šå¸¸ä¸ä¼šè§¦å‘
#    - å¦‚æœä»æœ‰é—®é¢˜ï¼Œå¢åŠ  max_retries åˆ° 5
#
# 4. å¦‚æœç¬¬ 3 æ‰¹å‡ºç° "No assistant text message found" é”™è¯¯ï¼š
#    - å¯èƒ½æ˜¯ API ä¸´æ—¶é—®é¢˜æˆ–é€Ÿç‡é™åˆ¶
#    - è„šæœ¬ä¼šè‡ªåŠ¨é‡è¯•ï¼ˆmax_retries = 3ï¼‰

# Notionå­—æ®µæ˜ å°„ï¼ˆè¯·æŒ‰ä½ çš„ Notion DB å±æ€§åæ”¹ï¼‰
# ä½  Notion DB é‡Œå»ºè®®è‡³å°‘å»ºè¿™äº›åˆ—ï¼ˆåå­—è¦ä¸€è‡´ï¼‰ï¼š
# Name(title), Company(rich_text), Site(select), Job URL(url), Date Posted(date),
# Score(number), Verdict(select), Reasons(rich_text), Gaps(rich_text), Strategy(rich_text),
# Keywords(rich_text), Risk(select)
NOTION_PROPS = {
    "å²—ä½åç§°": "å²—ä½åç§°",          # Title
    "å…¬å¸åç§°": "å…¬å¸åç§°",          # Rich text
    "æ‹›è˜å¹³å°": "æ‹›è˜å¹³å°",          # Select
    "å²—ä½é“¾æ¥": "å²—ä½é“¾æ¥",          # URL
    "å·¥ä½œåœ°ç‚¹": "å·¥ä½œåœ°ç‚¹",          # Rich text
    "å‘å¸ƒæ—¶é—´": "å‘å¸ƒæ—¶é—´",          # Date
    "åˆåŒç±»å‹ï¼ˆæ¨æ–­ï¼‰": "åˆåŒç±»å‹ï¼ˆæ¨æ–­ï¼‰",  # Select

    "æ˜¯å¦å€¼å¾—æŠ•é€’": "æ˜¯å¦å€¼å¾—æŠ•é€’",  # Select
    "åŒ¹é…è¯„åˆ†": "åŒ¹é…è¯„åˆ†",          # Number
    "é£é™©ç­‰çº§": "é£é™©ç­‰çº§",          # Select
    "æŠ•é€’ç­–ç•¥": "æŠ•é€’ç­–ç•¥",          # Rich text

    "åŒ¹é…åŸå› ": "åŒ¹é…åŸå› ",          # Rich text
    "ä¸»è¦ç¼ºå£": "ä¸»è¦ç¼ºå£",          # Rich text
    "å…³é”®è¯": "å…³é”®è¯",              # Multi-select
    "æ€»ä½“å»ºè®®": "æ€»ä½“å»ºè®®",          # Rich text

    "åŸå§‹åˆ†æ JSON": "åŸå§‹åˆ†æ JSON",  # Rich text
    "åˆ†ææ—¥æœŸ": "åˆ†ææ—¥æœŸ",            # Date
    "æ•°æ®æ¥æºæ‰¹æ¬¡": "æ•°æ®æ¥æºæ‰¹æ¬¡",    # Rich text
}



# -----------------------------
# 1) Helpers
# -----------------------------
def extract_json(text: str):
    """
    ä» assistant è¾“å‡ºä¸­å®‰å…¨æå– JSONï¼ˆæ”¯æŒ ```json``` åŒ…è£¹ã€å‰ååºŸè¯ï¼‰
    """
    if not text:
        raise ValueError("Empty assistant output")

    original_text = text
    text = text.strip()
    
    print(f"[DEBUG extract_json] åŸå§‹è¾“å‡ºé•¿åº¦: {len(text)} å­—ç¬¦")
    print(f"[DEBUG extract_json] å‰ 200 å­—ç¬¦: {text[:200]}")

    # å»æ‰ ```json ... ```
    text = re.sub(r"^```(?:json)?\s*", "", text)
    text = re.sub(r"\s*```$", "", text)

    # æŠ“ç¬¬ä¸€ä¸ª JSON æ•°ç»„æˆ–å¯¹è±¡
    m = re.search(r"(\[[\s\S]*\]|\{[\s\S]*\})", text)
    if not m:
        print(f"[ERROR extract_json] æœªæ‰¾åˆ° JSON ç»“æ„")
        print(f"[ERROR extract_json] å®Œæ•´è¾“å‡ºï¼ˆå‰ 1000 å­—ç¬¦ï¼‰: {original_text[:1000]}")
        raise ValueError(f"Assistant output is not JSON. preview={text[:300]}")

    json_str = m.group(1)
    print(f"[DEBUG extract_json] æå–åˆ°çš„ JSON é•¿åº¦: {len(json_str)} å­—ç¬¦")
    print(f"[DEBUG extract_json] JSON å¼€å¤´: {json_str[:100]}")
    
    # æ£€æµ‹æ˜¯å¦æ˜¯æ•°ç»„è¿˜æ˜¯å•ä¸ªå¯¹è±¡
    if json_str.strip().startswith('['):
        print(f"[DEBUG extract_json] æ£€æµ‹åˆ° JSON æ•°ç»„")
    elif json_str.strip().startswith('{'):
        print(f"[WARN extract_json] æ£€æµ‹åˆ°å•ä¸ª JSON å¯¹è±¡ï¼ˆè€Œéæ•°ç»„ï¼‰")
    
    return json.loads(json_str)
    
def ensure_assistant_has_cv_vector_store(client: OpenAI, assistant_id: str, vector_store_id: str):
    a = client.beta.assistants.retrieve(assistant_id)

    tr = a.tool_resources
    tr_dict = tr.model_dump() if tr else {}
    existing_vs = (tr_dict.get("file_search") or {}).get("vector_store_ids") or []

    if vector_store_id in existing_vs:
        print("[OK] Assistant already linked to CV vector store.")
        return

    tools = list(a.tools or [])
    if not any(
        getattr(t, "type", None) == "file_search" or (isinstance(t, dict) and t.get("type") == "file_search")
        for t in tools
    ):
        tools.append({"type": "file_search"})

    new_vs = list(dict.fromkeys(existing_vs + [vector_store_id]))

    client.beta.assistants.update(
        assistant_id=assistant_id,
        tools=tools,
        tool_resources={"file_search": {"vector_store_ids": new_vs}},
    )
    print("[OK] Assistant updated with CV vector store:", new_vs)

def normalize_results(parsed):
    """
    ç›®æ ‡ï¼šæœ€ç»ˆä¸€å®šè¿”å› List[Dict]
    å…¼å®¹ï¼š
    - parsed æ˜¯ strï¼ˆæ•´æ®µ JSON ä½œä¸ºå­—ç¬¦ä¸²ï¼‰
    - parsed æ˜¯ list[str]ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ JSON å­—ç¬¦ä¸²ï¼‰
    - parsed æ˜¯ dictï¼ˆåŒ…äº†ä¸€å±‚æˆ–å¤šå±‚ï¼‰
    """
    
    print(f"[DEBUG normalize_results] è¾“å…¥ç±»å‹: {type(parsed)}")

    # 1) å¦‚æœæ•´ä½“æ˜¯å­—ç¬¦ä¸²ï¼šå† loads ä¸€æ¬¡
    if isinstance(parsed, str):
        try:
            parsed = json.loads(parsed)
            print(f"[DEBUG normalize_results] å­—ç¬¦ä¸²è§£æåç±»å‹: {type(parsed)}")
        except Exception as e:
            raise ValueError(f"Parsed is a string but not JSON: {parsed[:200]}") from e

    # 2) å¦‚æœæ˜¯ list[str]ï¼šé€ä¸ª loads
    if isinstance(parsed, list) and (len(parsed) == 0 or isinstance(parsed[0], str)):
        new_list = []
        for i, s in enumerate(parsed):
            if not isinstance(s, str):
                raise ValueError(f"Mixed list types at index {i}: {type(s)}")
            try:
                new_list.append(json.loads(s))
            except Exception as e:
                raise ValueError(f"List item {i} is not JSON string. preview={s[:200]}") from e
        parsed = new_list
        print(f"[DEBUG normalize_results] list[str] è§£æå®Œæˆï¼Œå…± {len(parsed)} é¡¹")

    # 3) å¦‚æœå·²ç»æ˜¯ list[dict]ï¼šç›´æ¥è¿”å›
    if isinstance(parsed, list) and (len(parsed) == 0 or isinstance(parsed[0], dict)):
        print(f"[DEBUG normalize_results] å·²æ˜¯ list[dict]ï¼Œå…± {len(parsed)} é¡¹")
        return parsed

    # 4) å¦‚æœæ˜¯ dictï¼šåœ¨é‡Œé¢"é€’å½’"æ‰¾åˆ°ç¬¬ä¸€ä¸ª list[dict]
    if isinstance(parsed, dict):
        print(f"[DEBUG normalize_results] æ˜¯ dictï¼Œkeys: {list(parsed.keys())[:10]}")

        def find_list_of_dict(obj, depth=0, max_depth=6):
            if depth > max_depth:
                return None

            # ç›´æ¥å‘½ä¸­ï¼šlist[dict]
            if isinstance(obj, list) and obj and all(isinstance(x, dict) for x in obj):
                return obj

            # ç©ºåˆ—è¡¨ä¹Ÿç®—æœ‰æ•ˆç»“æœï¼ˆå¯èƒ½æ²¡æœ‰å²—ä½ï¼‰
            if isinstance(obj, list) and len(obj) == 0:
                return obj

            if isinstance(obj, dict):
                # å…ˆä¼˜å…ˆå¸¸è§ key
                for k in ("results", "data", "items", "output", "content", "analysis", "jobs"):
                    if k in obj:
                        got = find_list_of_dict(obj[k], depth + 1, max_depth)
                        if got is not None:
                            return got
                # å†éå†æ‰€æœ‰ value
                for v in obj.values():
                    got = find_list_of_dict(v, depth + 1, max_depth)
                    if got is not None:
                        return got
            return None

        found = find_list_of_dict(parsed)
        if found is not None:
            print(f"[DEBUG normalize_results] åœ¨ dict ä¸­æ‰¾åˆ° list[dict]ï¼Œå…± {len(found)} é¡¹")
            return found

        # å¦‚æœ dict æœ¬èº«å°±æ˜¯ä¸€ä¸ªå²—ä½å¯¹è±¡ï¼ˆAI åªè¿”å›äº† 1 ä¸ªå¯¹è±¡è€Œä¸æ˜¯æ•°ç»„ï¼‰
        if "job_url" in parsed:
            print(f"[WARN normalize_results] AI åªè¿”å›äº†å•ä¸ª dict å¯¹è±¡ï¼Œå°†å…¶åŒ…è£…æˆæ•°ç»„")
            return [parsed]

        raise ValueError(f"Dict parsed but cannot find list[dict]. keys={list(parsed.keys())[:20]}")

    raise ValueError(f"Expected list[dict], got {type(parsed)}")

def fetch_jobs() -> pd.DataFrame:
    jobs = scrape_jobs(
        site_name=JOBSPY_CONFIG["sites"],   # è¿™é‡Œä¼  listï¼Œè®© jobspy è‡ªå·±å¤šç«™ç‚¹æŠ“
        search_term=JOBSPY_CONFIG["search_term"],
        location=JOBSPY_CONFIG["location"],
        results_wanted=JOBSPY_CONFIG["results_wanted"],
        hours_old=JOBSPY_CONFIG["hours_old"],
        linkedin_fetch_description=JOBSPY_CONFIG.get("linkedin_fetch_description", False),
        description_format=JOBSPY_CONFIG.get("description_format", "markdown"),
        verbose=JOBSPY_CONFIG.get("verbose", 1),
    )

    keep_cols = [
        "id", "site", "title", "company", "location", "date_posted", "job_url",
        "job_url_direct", "description", "job_type", "job_level", "company_industry"
    ]
    for c in keep_cols:
        if c not in jobs.columns:
            jobs[c] = None
    return jobs[keep_cols].copy()


def jobs_df_to_payload(jobs: pd.DataFrame) -> List[Dict[str, Any]]:
    records: List[Dict[str, Any]] = []
    for _, row in jobs.iterrows():
        records.append({
            "job_id": row.get("id"),
            "site": row.get("site"),
            "title": row.get("title"),
            "company": row.get("company"),
            "location": row.get("location"),
            "date_posted": str(row.get("date_posted")) if row.get("date_posted") else None,
            "job_url": row.get("job_url"),
            "job_url_direct": row.get("job_url_direct"),
            "job_level": row.get("job_level"),
            "company_industry": row.get("company_industry"),
            "description": row.get("description") or "",
        })
    return records


def build_system_instructions() -> str:
    """
    æ„å»ºç³»ç»ŸæŒ‡ä»¤ï¼ˆåªéœ€å‘é€ä¸€æ¬¡ï¼‰
    """
    return """
ä½ æ˜¯ä¸€åæ¬§æ´²ï¼ˆæ³•å›½ï¼‰æŠ€æœ¯å²—ä½æ‹›è˜åˆ†æä¸“å®¶ï¼Œ
ä¸“æ³¨äºä»¥ä¸‹é¢†åŸŸï¼š
- Supply Chain / Logistics Engineering
- Project Management Consulting / Technical PM

ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå²—ä½åˆ—è¡¨å¹¶è¾“å‡º JSON æ•°ç»„ã€‚
å€™é€‰äººçš„å®Œæ•´ç®€å† **ä»…é€šè¿‡ file_search å·¥å…·æä¾›**ï¼ˆCV å·²å…¥åº“å¹¶ç»‘å®šåˆ°ä½ ï¼‰ã€‚

ã€è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼è¦æ±‚ï¼‰ã€‘
- å¿…é¡»è¾“å‡º JSON æ•°ç»„æ ¼å¼ï¼š[{...}, {...}, ...]
- å³ä½¿åªæœ‰ 1 ä¸ªå²—ä½ï¼Œä¹Ÿå¿…é¡»è¾“å‡ºæ•°ç»„ï¼š[{...}]
- ç¦æ­¢è¾“å‡ºå•ä¸ªå¯¹è±¡ {...}
- å¿…é¡»ç”¨æ–¹æ‹¬å· [] åŒ…è£¹

ã€è¾“å‡ºé•¿åº¦æ§åˆ¶ã€‘
- ç¦æ­¢é•¿æ®µè½ã€ç¦æ­¢é‡å¤è§£é‡Š
- æ¯ä¸ªå­—æ®µä¿æŒç®€æ´

ã€æ£€ç´¢è¦æ±‚ã€‘
- ä½ å¿…é¡»ä½¿ç”¨ file_search æ£€ç´¢å€™é€‰äººç®€å†ï¼Œå†è¿›è¡ŒåŒ¹é…ä¸åˆ¤æ–­
- ä¸å…è®¸å‡­ç©ºå‡è®¾å€™é€‰äººç»å†
- è‹¥ç®€å†é‡Œæ‰¾ä¸åˆ°å…³é”®ä¿¡æ¯ï¼Œå¿…é¡»åœ¨ gaps ä¸­æ˜ç¡®å†™"ç®€å†æœªä½“ç°ï¼šxxx"

ã€åˆ†ææ­¥éª¤ã€‘ï¼ˆåœ¨å†…éƒ¨æ€è€ƒï¼Œä½†ä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼‰
1. æŠ½å–å²—ä½å…³é”®ä¿¡æ¯(èŒä½åç§°ï¼Œå…¬å¸åï¼Œè–ªèµ„ï¼Œåœ°åŒºï¼ŒèŒè´£ï¼ŒæŠ€èƒ½è¦æ±‚ï¼Œè¯­è¨€è¦æ±‚ï¼ŒæœŸæœ›çš„å€™é€‰äººï¼ŒåˆåŒç±»å‹ç­‰)
2. åˆ¤æ–­å²—ä½çœŸå®æ€§ä¸æ¸…æ™°åº¦
3. ä¸å€™é€‰äººç®€å†è¿›è¡ŒåŒ¹é…
4. ç»™å‡ºæ˜ç¡®çš„"æ˜¯å¦å€¼å¾—æŠ•é€’"çš„ç»“è®º
5. è¾“å‡ºä¸¥æ ¼ç»“æ„åŒ– JSON

ã€åŒ¹é…è§„åˆ™ã€‘
- åŒ¹é…åº¦è¶…è¿‡60%ï¼Œå³ä¸º"æŠ•"
- 40%åˆ°60%ä¹‹é—´ï¼Œå³ä¸º"è°¨æ…æŠ•"
- ä½äº40%ï¼Œå³ä¸º"ä¸æŠ•"

ã€æ¯ä¸ªå²—ä½è¾“å‡º Schemaã€‘
{
  "job_url": "...",
  "job_title": "...",
  "company": "...",
  "location": "...",
  "contract_type_guess": "CDI|CDD|Freelance|Unknown|Reject",
  "salary_guess_eur_month_gross": null,
  "risk_flag": "low|medium|high",
  "score": 0,
  "verdict": "æŠ•|ä¸æŠ•|è°¨æ…æŠ•",
  "match_reasons": ["..."],
  "gaps": ["..."],
  "keywords": ["..."],
  "apply_strategy": "...",
  "overall_advice": "..."
}

æˆ‘å·²ç»ç†è§£äº†è¦æ±‚ã€‚ç°åœ¨è¯·å‘é€å²—ä½æ•°æ®ï¼Œæˆ‘ä¼šæŒ‰ç…§ä¸Šè¿°æ ¼å¼åˆ†æã€‚
""".strip()


def build_batch_prompt(jobs_payload: List[Dict[str, Any]], batch_num: int = None) -> str:
    """
    æ„å»ºå•æ‰¹å²—ä½æ•°æ®çš„ promptï¼ˆåœ¨åŒä¸€ä¸ªå¯¹è¯ä¸­ä½¿ç”¨ï¼‰
    """
    batch_info = f"ï¼ˆæ‰¹æ¬¡ {batch_num}ï¼‰" if batch_num else ""
    return f"""
è¯·åˆ†æä»¥ä¸‹ {len(jobs_payload)} ä¸ªå²—ä½{batch_info}ï¼Œè¾“å‡º JSON æ•°ç»„ [{{"..."}}, {{"..."}}, ...]:

{json.dumps(jobs_payload, ensure_ascii=False, indent=2)}

âš ï¸ è®°ä½ï¼šå¿…é¡»è¾“å‡ºåŒ…å« {len(jobs_payload)} ä¸ªå…ƒç´ çš„ JSON æ•°ç»„ï¼
""".strip()


def build_user_prompt(jobs_payload: List[Dict[str, Any]]) -> str:
    """
    æ„å»ºç²¾ç®€ç‰ˆ promptï¼Œå‡å°‘ token æ¶ˆè€—
    """
    # ç‰¹åˆ«å¤„ç†ï¼šå¦‚æœåªæœ‰ 1 ä¸ªå²—ä½ï¼Œä¹Ÿè¦å¼ºè°ƒè¾“å‡ºæ•°ç»„æ ¼å¼
    count = len(jobs_payload)
    format_note = f"è¾“å‡º JSON æ•°ç»„ [{count} ä¸ªå…ƒç´ ]ï¼Œå³ä½¿åªæœ‰ 1 ä¸ªä¹Ÿç”¨ [{{'...'}}]"
    
    return f"""
åˆ†æ {count} ä¸ªæ³•å›½ä¾›åº”é“¾/ç‰©æµå·¥ç¨‹å²—ä½ï¼Œåˆ¤æ–­æ˜¯å¦å€¼å¾—æŠ•é€’ã€‚

è¾“å‡ºï¼šJSON æ•°ç»„ [{{...}}, {{...}}, ...]ï¼ŒåŒ…å« {count} ä¸ªå…ƒç´ 
ä½¿ç”¨ file_search æ£€ç´¢å€™é€‰äºº CV è¿›è¡ŒåŒ¹é…ï¼ˆç®€å†å·²ç»‘å®šåˆ°ä½ ï¼‰

æ¯ä¸ªå²—ä½è¾“å‡ºï¼š
{{
  "job_url": "...",
  "job_title": "...",
  "company": "...",
  "location": "...",
  "contract_type_guess": "CDI|CDD|Freelance|Unknown",
  "salary_guess_eur_month_gross": null,
  "risk_flag": "low|medium|high",
  "score": 0-100,
  "verdict": "æŠ•|è°¨æ…æŠ•|ä¸æŠ•",
  "match_reasons": ["ç®€çŸ­åŸå› 1", "ç®€çŸ­åŸå› 2"],
  "gaps": ["ç®€å†ç¼ºå°‘xxx"],
  "keywords": ["å…³é”®è¯"],
  "apply_strategy": "1-2å¥å»ºè®®",
  "overall_advice": "1å¥è¯æ€»ç»“"
}}

è¯„åˆ†è§„åˆ™ï¼š>60æŠ•ï¼Œ40-60è°¨æ…æŠ•ï¼Œ<40ä¸æŠ•

å²—ä½æ•°æ®ï¼ˆ{count} ä¸ªï¼‰ï¼š
{json.dumps(jobs_payload, ensure_ascii=False)}

âš ï¸ å¿…é¡»è¾“å‡º {count} ä¸ªåˆ†æï¼æ ¼å¼ï¼š[{{...}}, {{...}}]
""".strip()

def build_daily_report_prompt(results: list[dict], today: str) -> str:
    # ç»Ÿè®¡æ•°æ®
    verdict_count = {"æŠ•": 0, "è°¨æ…æŠ•": 0, "ä¸æŠ•": 0}
    top_jobs = []
    
    for r in results:
        verdict = r.get("verdict", "è°¨æ…æŠ•")
        verdict_count[verdict] = verdict_count.get(verdict, 0) + 1
        if verdict == "æŠ•":
            top_jobs.append(r)
    
    # æŒ‰è¯„åˆ†æ’åºï¼Œå–å‰3
    top_jobs.sort(key=lambda x: x.get("score", 0), reverse=True)
    top_jobs = top_jobs[:3]
    
    return f"""
âš ï¸ é‡è¦ï¼šè¿™æ˜¯ä¸€ä¸ªå†™é‚®ä»¶çš„ä»»åŠ¡ï¼Œä¸æ˜¯æ•°æ®åˆ†æä»»åŠ¡ï¼

ä½ çš„ä»»åŠ¡ï¼šå†™ä¸€å°ç»™ç”¨æˆ·çœ‹çš„ã€ä»Šæ—¥å²—ä½å°æŠ¥å‘Šã€‘é‚®ä»¶ã€‚
è¾“å‡ºè¦æ±‚ï¼šçº¯æ–‡æœ¬ã€è‡ªç„¶è¯­è¨€ã€äººç±»å¯ç›´æ¥é˜…è¯»ã€‚
ç»å¯¹ç¦æ­¢ï¼šä¸è¦è¾“å‡º JSONã€ä¸è¦è¾“å‡ºä»£ç ã€ä¸è¦è¾“å‡ºä»»ä½• {{"key": "value"}} æ ¼å¼ã€‚

--------------------
é‚®ä»¶æ­£æ–‡è¦æ±‚ï¼š
--------------------

æ ‡é¢˜è¡Œï¼šğŸ“Œ ä»Šæ—¥å²—ä½å°æŠ¥å‘Š | {today}

ç¬¬ä¸€æ®µï¼šç”¨ä¸€å¥è¯æ€»ç»“ä»Šå¤©çš„å²—ä½æƒ…å†µ
ä¾‹å¦‚ï¼š"ä»Šå¤©ä¸ºä½ åˆ†æäº† {len(results)} ä¸ªä¾›åº”é“¾ç›¸å…³å²—ä½ï¼Œå…¶ä¸­ {verdict_count.get('æŠ•', 0)} ä¸ªå€¼å¾—æŠ•é€’ï¼Œ{verdict_count.get('è°¨æ…æŠ•', 0)} ä¸ªå»ºè®®è°¨æ…æŠ•é€’ï¼Œ{verdict_count.get('ä¸æŠ•', 0)} ä¸ªä¸å»ºè®®æŠ•é€’ã€‚"

ç¬¬äºŒéƒ¨åˆ†ï¼šğŸŒŸ æœ€å€¼å¾—æŠ•çš„ Top 3
åˆ—å‡º 3 ä¸ªæœ€æ¨èçš„å²—ä½ï¼Œæ¯ä¸ªå²—ä½åŒ…å«ï¼š
- å²—ä½åç§° | å…¬å¸ | åœ°ç‚¹
- æ¨èåŸå› ï¼ˆ1-2å¥è¯ï¼‰
- ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®ï¼ˆ1å¥è¯ï¼‰
- å²—ä½é“¾æ¥

ç¬¬ä¸‰éƒ¨åˆ†ï¼šâš ï¸ è°¨æ…æŠ•é€’æé†’
å¦‚æœæœ‰è°¨æ…æŠ•çš„å²—ä½ï¼Œç”¨ 1-2 æ®µè¯æ€»ç»“å…±æ€§é—®é¢˜ã€‚

ç¬¬å››éƒ¨åˆ†ï¼šâŒ ä¸å»ºè®®æŠ•é€’çš„åŸå› 
å¦‚æœæœ‰ä¸å»ºè®®æŠ•çš„å²—ä½ï¼Œç”¨ 1 æ®µè¯æ€»ç»“åŸå› ã€‚

ç¬¬äº”éƒ¨åˆ†ï¼šğŸ“ ä»Šæ—¥è¡ŒåŠ¨æ¸…å•
åˆ—å‡º 3-6 æ¡å…·ä½“å¯æ‰§è¡Œçš„å»ºè®®ï¼Œä¾‹å¦‚ï¼š
1. å®šåˆ¶ç®€å†ï¼Œçªå‡ºæŸæŸç»éªŒ
2. è¡¥å……æŸæŸèƒ½åŠ›
3. ...

ç»“å°¾ï¼šä¸€å¥é¼“åŠ±çš„è¯ã€‚

--------------------
ç°åœ¨å¼€å§‹å†™é‚®ä»¶æ­£æ–‡ï¼ˆç›´æ¥å¼€å§‹ï¼Œä¸è¦ä»»ä½•å‰ç½®è¯´æ˜ï¼‰ï¼š

å‚è€ƒæ•°æ®ï¼š
ä»Šå¤©æ—¥æœŸï¼š{today}
æ€»å²—ä½æ•°ï¼š{len(results)}
å€¼å¾—æŠ•ï¼š{verdict_count.get('æŠ•', 0)} ä¸ª
è°¨æ…æŠ•ï¼š{verdict_count.get('è°¨æ…æŠ•', 0)} ä¸ª
ä¸å»ºè®®æŠ•ï¼š{verdict_count.get('ä¸æŠ•', 0)} ä¸ª

Top 3 å²—ä½ï¼š
{json.dumps(top_jobs, ensure_ascii=False, indent=2)}

æ‰€æœ‰å²—ä½ï¼š
{json.dumps(results, ensure_ascii=False, indent=2)}

âš ï¸ å†æ¬¡æé†’ï¼šä¸è¦è¾“å‡º JSONï¼ç›´æ¥å†™é‚®ä»¶æ­£æ–‡ï¼ä»æ ‡é¢˜"ğŸ“Œ ä»Šæ—¥å²—ä½å°æŠ¥å‘Š"å¼€å§‹ï¼
""".strip()


# -----------------------------
# 2) OpenAI Assistant call (batch)
# -----------------------------
def run_assistant_in_thread(
    client: OpenAI,
    thread_id: str,
    assistant_id: str,
    message_content: str,
    expected_jobs: Optional[int] = None,
    max_retries: int = 3,
) -> tuple[List[Dict[str, Any]], str]:
    """
    åœ¨å·²æœ‰çš„ thread ä¸­å‘é€æ¶ˆæ¯å¹¶è·å–å“åº”
    """
    for attempt in range(max_retries):
        try:
            # åœ¨å·²æœ‰ thread ä¸­æ·»åŠ æ¶ˆæ¯
            client.beta.threads.messages.create(
                thread_id=thread_id,
                role="user",
                content=message_content
            )

            run = client.beta.threads.runs.create(
                thread_id=thread_id,
                assistant_id=assistant_id,
            )

            while True:
                run = client.beta.threads.runs.retrieve(
                    thread_id=thread_id,
                    run_id=run.id
                )
                if run.status in ("completed", "failed", "cancelled", "expired"):
                    break
                time.sleep(1.2)

            # å¤„ç† rate limit é”™è¯¯ï¼Œè‡ªåŠ¨é‡è¯•
            if run.status == "failed" and run.last_error:
                error_code = run.last_error.code if hasattr(run.last_error, 'code') else str(run.last_error)
                error_msg = run.last_error.message if hasattr(run.last_error, 'message') else str(run.last_error)
                
                if error_code == "rate_limit_exceeded":
                    # ä»é”™è¯¯æ¶ˆæ¯ä¸­æå–ç­‰å¾…æ—¶é—´
                    import re
                    wait_match = re.search(r"try again in ([\d.]+)s", error_msg)
                    wait_time = float(wait_match.group(1)) if wait_match else 15
                    
                    if attempt < max_retries - 1:
                        print(f"[WARN] é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•... (å°è¯• {attempt+1}/{max_retries})")
                        time.sleep(wait_time + 2)
                        continue
                    else:
                        raise RuntimeError(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œé€Ÿç‡é™åˆ¶é”™è¯¯: {error_msg}")
                else:
                    raise RuntimeError(f"Run failed: status={run.status}, error={error_code}: {error_msg}")
            
            if run.status != "completed":
                raise RuntimeError(f"Run not completed: status={run.status}, last_error={run.last_error}")
            
            # è®°å½• run çš„ token ä½¿ç”¨æƒ…å†µ
            if hasattr(run, 'usage') and run.usage:
                print(f"[DEBUG] Token ä½¿ç”¨: prompt={run.usage.prompt_tokens}, "
                      f"completion={run.usage.completion_tokens}, "
                      f"total={run.usage.total_tokens}")
            
            # æˆåŠŸå®Œæˆï¼Œè·³å‡ºé‡è¯•å¾ªç¯
            break
            
        except RuntimeError as e:
            error_str = str(e).lower()
            
            # å¯é‡è¯•çš„é”™è¯¯ç±»å‹
            if "rate_limit" in error_str or "no assistant text message found" in error_str:
                if attempt < max_retries - 1:
                    wait_time = 5 + attempt * 2
                    print(f"[WARN] é‡åˆ°é”™è¯¯: {str(e)[:100]}")
                    print(f"[WARN] ç­‰å¾… {wait_time} ç§’åé‡è¯•... (å°è¯• {attempt+1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"[ERROR] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œé”™è¯¯: {e}")
                    raise
            else:
                # å…¶ä»–ç±»å‹é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                raise

    msgs = client.beta.threads.messages.list(thread_id=thread_id)

    print(f"[DEBUG] Thread ä¸­å…±æœ‰ {len(msgs.data)} æ¡æ¶ˆæ¯")
    
    assistant_text = None
    for m in reversed(msgs.data):
        print(f"[DEBUG] æ¶ˆæ¯è§’è‰²: {m.role}, content blocks: {len(m.content)}")
        if m.role != "assistant":
            continue
        text = ""
        for block in m.content:
            print(f"[DEBUG]   Block type: {block.type}")
            if block.type == "text":
                text += block.text.value
            if text.strip():
                assistant_text = text
                break  # åªå–æœ€æ–°çš„ä¸€æ¡
        if assistant_text:
            break
    
    if not assistant_text:
        print(f"[ERROR] Run çŠ¶æ€: {run.status}")
        print(f"[ERROR] Run usage: {run.usage if hasattr(run, 'usage') else 'N/A'}")
        print(f"[ERROR] æœªæ‰¾åˆ° assistant æ–‡æœ¬æ¶ˆæ¯ï¼")
        print(f"[ERROR] Thread ID: {thread_id}, Run ID: {run.id}")
        
        for i, m in enumerate(msgs.data):
            print(f"[ERROR] Message {i}: role={m.role}, content_count={len(m.content)}")
            if m.role == "assistant":
                for j, block in enumerate(m.content):
                    print(f"[ERROR]   Block {j}: type={block.type}")
                    if block.type == "text":
                        print(f"[ERROR]   Text preview: {block.text.value[:200] if hasattr(block.text, 'value') else 'N/A'}")
        
        raise RuntimeError(f"No assistant text message found. Thread: {thread_id}, Run: {run.id}")

    print(f"[AI] æ”¶åˆ°åŸå§‹è¾“å‡ºï¼Œé•¿åº¦: {len(assistant_text)} å­—ç¬¦")

    parsed = extract_json(assistant_text)
    results = normalize_results(parsed)
    if expected_jobs is not None and len(results) != expected_jobs:
        print(f"[WARN] æœŸæœ› {expected_jobs} æ¡åˆ†æï¼Œå®é™…å¾—åˆ° {len(results)} æ¡")
    
    return results, assistant_text


def run_assistant_analysis(
    client: OpenAI,
    assistant_id: str,
    user_prompt: str,
    expected_jobs: Optional[int] = None,
    max_retries: int = 3,
) -> tuple[List[Dict[str, Any]], str]:
    """
    è¿”å›: (results: List[Dict], raw_text: str)
    æ”¯æŒè‡ªåŠ¨é‡è¯•ï¼ˆrate limit é”™è¯¯ï¼‰
    """
    for attempt in range(max_retries):
        try:
            thread = client.beta.threads.create(
                messages=[{
                    "role": "user",
                    "content": user_prompt
                }]
            )

            run = client.beta.threads.runs.create(
                thread_id=thread.id,
                assistant_id=assistant_id,
            )

            while True:
                run = client.beta.threads.runs.retrieve(
                    thread_id=thread.id,
                    run_id=run.id
                )
                if run.status in ("completed", "failed", "cancelled", "expired"):
                    break
                time.sleep(1.2)

            # å¤„ç† rate limit é”™è¯¯ï¼Œè‡ªåŠ¨é‡è¯•
            if run.status == "failed" and run.last_error:
                error_code = run.last_error.code if hasattr(run.last_error, 'code') else str(run.last_error)
                error_msg = run.last_error.message if hasattr(run.last_error, 'message') else str(run.last_error)
                
                if error_code == "rate_limit_exceeded":
                    # ä»é”™è¯¯æ¶ˆæ¯ä¸­æå–ç­‰å¾…æ—¶é—´
                    import re
                    wait_match = re.search(r"try again in ([\d.]+)s", error_msg)
                    wait_time = float(wait_match.group(1)) if wait_match else 15
                    
                    if attempt < max_retries - 1:
                        print(f"[WARN] é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•... (å°è¯• {attempt+1}/{max_retries})")
                        time.sleep(wait_time + 2)  # å¤šç­‰ 2 ç§’ç¡®ä¿å®‰å…¨
                        continue
                    else:
                        raise RuntimeError(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œé€Ÿç‡é™åˆ¶é”™è¯¯: {error_msg}")
                else:
                    raise RuntimeError(f"Run failed: status={run.status}, error={error_code}: {error_msg}")
            
            if run.status != "completed":
                raise RuntimeError(f"Run not completed: status={run.status}, last_error={run.last_error}")
            
            # è®°å½• run çš„ token ä½¿ç”¨æƒ…å†µ
            if hasattr(run, 'usage') and run.usage:
                print(f"[DEBUG] Token ä½¿ç”¨: prompt={run.usage.prompt_tokens}, "
                      f"completion={run.usage.completion_tokens}, "
                      f"total={run.usage.total_tokens}")
            
            # æˆåŠŸå®Œæˆï¼Œè·³å‡ºé‡è¯•å¾ªç¯
            break
            
        except RuntimeError as e:
            error_str = str(e).lower()
            
            # å¯é‡è¯•çš„é”™è¯¯ç±»å‹
            if "rate_limit" in error_str or "no assistant text message found" in error_str:
                if attempt < max_retries - 1:
                    wait_time = 5 + attempt * 2  # é€æ¸å¢åŠ ç­‰å¾…æ—¶é—´ï¼š5s, 7s, 9s
                    print(f"[WARN] é‡åˆ°é”™è¯¯: {str(e)[:100]}")
                    print(f"[WARN] ç­‰å¾… {wait_time} ç§’åé‡è¯•... (å°è¯• {attempt+1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"[ERROR] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œé”™è¯¯: {e}")
                    raise
            else:
                # å…¶ä»–ç±»å‹é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                raise

    msgs = client.beta.threads.messages.list(thread_id=thread.id)

    print(f"[DEBUG] Thread ä¸­å…±æœ‰ {len(msgs.data)} æ¡æ¶ˆæ¯")
    
    assistant_text = None
    for m in reversed(msgs.data):  # ä»æœ€æ—©åˆ°æœ€æ–°æ‰«ï¼Œæœ€åä¸€ä¸ª assistant_text ä¼šæ˜¯æœ€æ–°
        print(f"[DEBUG] æ¶ˆæ¯è§’è‰²: {m.role}, content blocks: {len(m.content)}")
        if m.role != "assistant":
            continue
        text = ""
        for block in m.content:
            print(f"[DEBUG]   Block type: {block.type}")
            if block.type == "text":
                text += block.text.value
            if text.strip():
                assistant_text = text  # ä¸ returnï¼Œç»§ç»­ï¼Œè®©å®ƒè¢«æœ€æ–°çš„è¦†ç›–
    
    if not assistant_text:
        print(f"[ERROR] Run çŠ¶æ€: {run.status}")
        print(f"[ERROR] Run usage: {run.usage if hasattr(run, 'usage') else 'N/A'}")
        print(f"[ERROR] æœªæ‰¾åˆ° assistant æ–‡æœ¬æ¶ˆæ¯ï¼")
        print(f"[ERROR] Thread ID: {thread.id}, Run ID: {run.id}")
        
        # å°è¯•è·å–æ›´å¤šä¿¡æ¯
        for i, m in enumerate(msgs.data):
            print(f"[ERROR] Message {i}: role={m.role}, content_count={len(m.content)}")
            if m.role == "assistant":
                for j, block in enumerate(m.content):
                    print(f"[ERROR]   Block {j}: type={block.type}")
                    if block.type == "text":
                        print(f"[ERROR]   Text preview: {block.text.value[:200] if hasattr(block.text, 'value') else 'N/A'}")
        
        raise RuntimeError(f"No assistant text message found. Thread: {thread.id}, Run: {run.id}")

    print(f"[AI] æ”¶åˆ°åŸå§‹è¾“å‡ºï¼Œé•¿åº¦: {len(assistant_text)} å­—ç¬¦")

    parsed = extract_json(assistant_text)
    results = normalize_results(parsed)
    if expected_jobs is not None and len(results) != expected_jobs:
        print(f"[WARN] æœŸæœ› {expected_jobs} æ¡åˆ†æï¼Œå®é™…å¾—åˆ° {len(results)} æ¡")
    
    return results, assistant_text


def generate_fallback_report(results: list[dict], today: str) -> str:
    """
    å½“ AI ç”ŸæˆæŠ¥å‘Šå¤±è´¥æ—¶ï¼Œç”Ÿæˆå¤‡ç”¨ç®€åŒ–æŠ¥å‘Š
    """
    verdict_count = {"æŠ•": 0, "è°¨æ…æŠ•": 0, "ä¸æŠ•": 0}
    top_jobs = []
    
    for r in results:
        verdict = r.get("verdict", "è°¨æ…æŠ•")
        verdict_count[verdict] = verdict_count.get(verdict, 0) + 1
        if verdict == "æŠ•":
            top_jobs.append(r)
    
    # æŒ‰è¯„åˆ†æ’åº
    top_jobs.sort(key=lambda x: x.get("score", 0), reverse=True)
    
    lines = []
    lines.append("=" * 60)
    lines.append(f"ğŸ“Œ ä»Šæ—¥å²—ä½å°æŠ¥å‘Š | {today}")
    lines.append("=" * 60)
    lines.append("")
    lines.append(f"ä»Šå¤©ä¸ºä½ åˆ†æäº† {len(results)} ä¸ªä¾›åº”é“¾ç›¸å…³å²—ä½ï¼š")
    lines.append(f"â€¢ å€¼å¾—æŠ•é€’ï¼š{verdict_count['æŠ•']} ä¸ª")
    lines.append(f"â€¢ è°¨æ…æŠ•é€’ï¼š{verdict_count['è°¨æ…æŠ•']} ä¸ª")
    lines.append(f"â€¢ ä¸å»ºè®®æŠ•é€’ï¼š{verdict_count['ä¸æŠ•']} ä¸ª")
    lines.append("")
    lines.append("-" * 60)
    lines.append("")
    lines.append("ğŸŒŸ æœ€å€¼å¾—æŠ•çš„å²—ä½")
    lines.append("")
    
    for i, job in enumerate(top_jobs[:3], 1):
        lines.append(f"{i}. {job.get('job_title', 'æœªçŸ¥å²—ä½')} | {job.get('company', 'æœªçŸ¥å…¬å¸')} | {job.get('location', 'æœªçŸ¥åœ°ç‚¹')}")
        lines.append(f"   è¯„åˆ†: {job.get('score', 0)}")
        
        match_reasons = job.get('match_reasons', [])
        if match_reasons and len(match_reasons) > 0:
            lines.append(f"   æ¨èåŸå› : {match_reasons[0]}")
        
        lines.append(f"   é“¾æ¥: {job.get('job_url', '')}")
        lines.append("")
    
    lines.append("-" * 60)
    lines.append("")
    lines.append("ğŸ“ å»ºè®®")
    lines.append("")
    lines.append("1. ä¼˜å…ˆå…³æ³¨è¯„åˆ†è¾ƒé«˜çš„å²—ä½")
    lines.append("2. å®šåˆ¶ç®€å†ï¼Œçªå‡ºåŒ¹é…çš„æŠ€èƒ½å’Œç»éªŒ")
    lines.append("3. å‡†å¤‡é’ˆå¯¹æ€§çš„ Cover Letter")
    lines.append("")
    lines.append("=" * 60)
    lines.append("ğŸ’¬ ç¥ä½ æ±‚èŒé¡ºåˆ©ï¼")
    lines.append("=" * 60)
    
    return "\n".join(lines)


def convert_json_report_to_text(json_or_text: str, results: list[dict], today: str) -> str:
    """
    å¦‚æœ AI è¿”å›çš„æ˜¯ JSON æ ¼å¼ï¼Œå°†å…¶è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æ–‡æœ¬æŠ¥å‘Šã€‚
    å¦‚æœå·²ç»æ˜¯æ–‡æœ¬æ ¼å¼ï¼Œç›´æ¥è¿”å›ã€‚
    """
    text = json_or_text.strip()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ JSON æ ¼å¼
    if text.startswith('{') or (text.startswith('```') and 'json' in text[:20].lower()):
        print("[WARN] AI è¿”å›äº† JSON æ ¼å¼ï¼Œæ­£åœ¨è‡ªåŠ¨è½¬æ¢ä¸ºç¾åŒ–æ–‡æœ¬...")
        
        # å»æ‰ markdown ä»£ç å—
        text = re.sub(r'^```(?:json)?\s*', '', text)
        text = re.sub(r'\s*```$', '', text)
        
        try:
            data = json.loads(text)
        except:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸæ–‡æœ¬
            print("[WARN] JSON è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬")
            return json_or_text
        
        # æ‰‹åŠ¨æ„å»ºç¾åŒ–çš„æ–‡æœ¬æŠ¥å‘Š
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append(f"ğŸ“Œ ä»Šæ—¥å²—ä½å°æŠ¥å‘Š | {today}")
        report_lines.append("=" * 60)
        report_lines.append("")
        
        # æ€»è§ˆ
        overview = data.get("2ï¸âƒ£ ä»Šæ—¥å²—ä½æ€»è§ˆ", data.get("ä»Šæ—¥å²—ä½æ€»è§ˆ", ""))
        if overview:
            report_lines.append(overview)
        else:
            verdict_count = {"æŠ•": 0, "è°¨æ…æŠ•": 0, "ä¸æŠ•": 0}
            for r in results:
                verdict = r.get("verdict", "è°¨æ…æŠ•")
                verdict_count[verdict] = verdict_count.get(verdict, 0) + 1
            report_lines.append(f"ä»Šå¤©ä¸ºä½ åˆ†æäº† {len(results)} ä¸ªä¾›åº”é“¾ç›¸å…³å²—ä½ï¼Œ"
                              f"å…¶ä¸­ {verdict_count['æŠ•']} ä¸ªå€¼å¾—æŠ•é€’ï¼Œ"
                              f"{verdict_count['è°¨æ…æŠ•']} ä¸ªå»ºè®®è°¨æ…æŠ•é€’ï¼Œ"
                              f"{verdict_count['ä¸æŠ•']} ä¸ªä¸å»ºè®®æŠ•é€’ã€‚")
        report_lines.append("")
        report_lines.append("-" * 60)
        
        # Top 3
        report_lines.append("")
        report_lines.append("ğŸŒŸ æœ€å€¼å¾—æŠ•çš„ Top 3")
        report_lines.append("")
        top3 = data.get("3ï¸âƒ£ ğŸŒŸ æœ€å€¼å¾—æŠ•çš„ Top 3", data.get("æœ€å€¼å¾—æŠ•çš„ Top 3", ""))
        if top3:
            # å¤„ç†æ¢è¡Œï¼Œç¡®ä¿æ ¼å¼æ•´é½
            report_lines.append(top3.strip())
        report_lines.append("")
        report_lines.append("-" * 60)
        
        # è°¨æ…æŠ•æé†’
        report_lines.append("")
        report_lines.append("âš ï¸ è°¨æ…æŠ•é€’æé†’")
        report_lines.append("")
        caution = data.get("4ï¸âƒ£ âš ï¸ è°¨æ…æŠ•çš„å²—ä½å…±æ€§æé†’", data.get("è°¨æ…æŠ•çš„å²—ä½å…±æ€§æé†’", ""))
        if caution:
            report_lines.append(caution.strip())
        else:
            report_lines.append("æœ¬æ¬¡åˆ†æçš„è°¨æ…æŠ•é€’å²—ä½éœ€è¦ç‰¹åˆ«æ³¨æ„è¡Œä¸šèƒŒæ™¯å’Œè¯­è¨€èƒ½åŠ›è¦æ±‚ã€‚")
        report_lines.append("")
        report_lines.append("-" * 60)
        
        # ä¸å»ºè®®æŠ•
        report_lines.append("")
        report_lines.append("âŒ ä¸å»ºè®®æŠ•é€’çš„åŸå› ")
        report_lines.append("")
        no_apply = data.get("5ï¸âƒ£ âŒ ä¸å»ºè®®æŠ•çš„ä¸»è¦åŸå› æ€»ç»“", data.get("ä¸å»ºè®®æŠ•çš„ä¸»è¦åŸå› æ€»ç»“", ""))
        if no_apply:
            report_lines.append(no_apply.strip())
        else:
            report_lines.append("éƒ¨åˆ†å²—ä½å› è¡Œä¸šé—¨æ§›æˆ–ç»éªŒè¦æ±‚ä¸å½“å‰èƒŒæ™¯å·®è·è¾ƒå¤§ï¼Œå»ºè®®ä¼˜å…ˆå…³æ³¨åŒ¹é…åº¦æ›´é«˜çš„æœºä¼šã€‚")
        report_lines.append("")
        report_lines.append("-" * 60)
        
        # è¡ŒåŠ¨æ¸…å•
        report_lines.append("")
        report_lines.append("ğŸ“ ä»Šæ—¥è¡ŒåŠ¨æ¸…å•")
        report_lines.append("")
        actions = data.get("6ï¸âƒ£ ğŸ“ ä»Šå¤©çš„è¡ŒåŠ¨æ¸…å•", data.get("ä»Šå¤©çš„è¡ŒåŠ¨æ¸…å•", ""))
        if actions:
            if isinstance(actions, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼ŒæŒ‰æ¢è¡Œæˆ–ç¼–å·åˆ†å‰²
                action_lines = actions.strip().split('\n')
                for line in action_lines:
                    line = line.strip()
                    if line:
                        # å¦‚æœå·²ç»æœ‰ç¼–å·ï¼Œç›´æ¥ç”¨ï¼›å¦åˆ™æ·»åŠ ç¼–å·
                        if re.match(r'^\d+[\.\)ã€]', line):
                            report_lines.append(line)
                        else:
                            report_lines.append(f"â€¢ {line}")
            elif isinstance(actions, list):
                for i, action in enumerate(actions, 1):
                    report_lines.append(f"{i}. {action}")
        else:
            report_lines.append("1. å®šåˆ¶ç®€å†ï¼Œçªå‡ºæ ¸å¿ƒæŠ€èƒ½å’Œé¡¹ç›®ç»éªŒ")
            report_lines.append("2. å…³æ³¨æœ€åŒ¹é…å²—ä½çš„å…¬å¸åŠ¨æ€")
            report_lines.append("3. å‡†å¤‡é’ˆå¯¹æ€§çš„ Cover Letter")
        report_lines.append("")
        report_lines.append("=" * 60)
        
        # ç»“å°¾
        report_lines.append("")
        ending = data.get("7ï¸âƒ£ ğŸ’¬ ç»“å°¾ä¸€å¥ç®€çŸ­æé†’", data.get("ç»“å°¾ä¸€å¥ç®€çŸ­æé†’", ""))
        if ending:
            report_lines.append(f"ğŸ’¬ {ending.strip()}")
        else:
            report_lines.append("ğŸ’¬ ç¥ä½ æ±‚èŒé¡ºåˆ©ï¼æŠ“ä½æ ¸å¿ƒåŒ¹é…å²—ä½ï¼Œæå‡æ¯æ¬¡æŠ•é€’çš„è½¬åŒ–ç‡ã€‚")
        report_lines.append("")
        report_lines.append("=" * 60)
        
        print("[OK] JSON å·²æˆåŠŸè½¬æ¢ä¸ºç¾åŒ–æ–‡æœ¬æ ¼å¼")
        return "\n".join(report_lines)
    
    # å·²ç»æ˜¯æ–‡æœ¬æ ¼å¼
    return json_or_text


def run_daily_report_text(client: OpenAI, assistant_id: str, results: List[Dict[str, Any]]) -> str:
    """
    ç”Ÿæˆæ¯æ—¥å²—ä½å°æŠ¥å‘Š
    """
    print("[æŠ¥å‘Š] å¼€å§‹ç”Ÿæˆæ¯æ—¥å²—ä½å°æŠ¥å‘Š...")
    today = datetime.now().strftime("%Y-%m-%d")
    prompt = build_daily_report_prompt(results, today)
    
    print(f"[æŠ¥å‘Š] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")

    try:
        thread = client.beta.threads.create(messages=[{"role": "user", "content": prompt}])
        print(f"[æŠ¥å‘Š] Thread åˆ›å»ºæˆåŠŸ: {thread.id}")
        
        run = client.beta.threads.runs.create(thread_id=thread.id, assistant_id=assistant_id)
        print(f"[æŠ¥å‘Š] Run åˆ›å»ºæˆåŠŸ: {run.id}")

        while True:
            run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)
            if run.status in ("completed", "failed", "cancelled", "expired"):
                break
            time.sleep(1.2)

        print(f"[æŠ¥å‘Š] Run çŠ¶æ€: {run.status}")
        
        if run.status != "completed":
            error_msg = f"status={run.status}, last_error={run.last_error}"
            print(f"[ERROR æŠ¥å‘Š] {error_msg}")
            raise RuntimeError(f"Daily report run failed: {error_msg}")

        msgs = client.beta.threads.messages.list(thread_id=thread.id)
        print(f"[æŠ¥å‘Š] Thread ä¸­å…±æœ‰ {len(msgs.data)} æ¡æ¶ˆæ¯")

        # æ‹¿æœ€æ–°ä¸€æ¡ assistant æ–‡æœ¬
        assistant_text = None
        for m in reversed(msgs.data):
            print(f"[æŠ¥å‘Š] æ¶ˆæ¯è§’è‰²: {m.role}, content blocks: {len(m.content)}")
            if m.role != "assistant":
                continue
            text = ""
            for block in m.content:
                print(f"[æŠ¥å‘Š]   Block type: {block.type}")
                if block.type == "text":
                    text += block.text.value
            if text.strip():
                assistant_text = text
                print(f"[æŠ¥å‘Š] æ‰¾åˆ° assistant æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
                break

        if not assistant_text:
            print(f"[ERROR æŠ¥å‘Š] æœªæ‰¾åˆ° assistant æ–‡æœ¬æ¶ˆæ¯ï¼")
            print(f"[ERROR æŠ¥å‘Š] Thread ID: {thread.id}, Run ID: {run.id}")
            
            # å°è¯•ä»æ‰€æœ‰æ¶ˆæ¯ä¸­æ‰¾åˆ°ä»»ä½•æ–‡æœ¬
            for i, m in enumerate(msgs.data):
                print(f"[ERROR æŠ¥å‘Š] Message {i}: role={m.role}")
                for j, block in enumerate(m.content):
                    if hasattr(block, 'text') and hasattr(block.text, 'value'):
                        print(f"[ERROR æŠ¥å‘Š]   Text preview: {block.text.value[:200]}")
            
            # ç”Ÿæˆå¤‡ç”¨æŠ¥å‘Š
            print("[æŠ¥å‘Š] ç”Ÿæˆå¤‡ç”¨ç®€åŒ–æŠ¥å‘Š...")
            return generate_fallback_report(results, today)

        print(f"[æŠ¥å‘Š] AI è¿”å›æ–‡æœ¬å‰ 200 å­—ç¬¦: {assistant_text[:200]}")
        
        # å¦‚æœæ˜¯ JSON æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ–‡æœ¬
        final_text = convert_json_report_to_text(assistant_text.strip(), results, today)
        print(f"[æŠ¥å‘Š] æœ€ç»ˆæŠ¥å‘Šé•¿åº¦: {len(final_text)} å­—ç¬¦")
        return final_text
        
    except Exception as e:
        print(f"[ERROR æŠ¥å‘Š] ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        print(f"[æŠ¥å‘Š] ä½¿ç”¨å¤‡ç”¨æŠ¥å‘Š...")
        return generate_fallback_report(results, today)


def text_to_simple_html(text: str) -> str:
    escaped = (
        text.replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
    )
    # æ¢è¡Œå˜ <br>
    html = escaped.replace("\n", "<br>")
    return f"<html><body style='font-family:Arial,Helvetica,sans-serif;line-height:1.5'>{html}</body></html>"


# -----------------------------
# Graph é‚®ä»¶å‘é€ï¼ˆDevice Code Flow + /me/sendMailï¼‰
# -----------------------------
GRAPH_SCOPES = ["User.Read", "Mail.Send"]  # delegated scopes for Graph
TOKEN_CACHE_PATH = Path("ms_token_cache.bin")


def _load_cache() -> msal.SerializableTokenCache:
    cache = msal.SerializableTokenCache()
    if TOKEN_CACHE_PATH.exists():
        cache.deserialize(TOKEN_CACHE_PATH.read_text(encoding="utf-8"))
    return cache


def _save_cache(cache: msal.SerializableTokenCache) -> None:
    if cache.has_state_changed:
        TOKEN_CACHE_PATH.write_text(cache.serialize(), encoding="utf-8")


def get_graph_access_token() -> str:
    """
    Device Code Flow:
    - ç¬¬ä¸€æ¬¡è¿è¡Œï¼šä¼šæ‰“å°éªŒè¯ç +ç™»å½•é“¾æ¥ï¼Œä½ åœ¨æµè§ˆå™¨å®Œæˆç™»å½•æˆæƒ
    - ä¹‹åè¿è¡Œï¼šä¼˜å…ˆèµ°ç¼“å­˜ï¼Œæ— éœ€äº¤äº’ï¼ˆé€‚åˆ Task Schedulerï¼‰
    """
    client_id = os.getenv("MS_CLIENT_ID")
    authority = os.getenv("MS_AUTHORITY", "https://login.microsoftonline.com/consumers")
    if not client_id:
        raise RuntimeError("Missing env var: MS_CLIENT_ID")
    if not authority:
        raise RuntimeError("Missing env var: MS_AUTHORITY")

    cache = _load_cache()
    app = msal.PublicClientApplication(client_id=client_id, authority=authority, token_cache=cache)

    # 1) å…ˆé™é»˜å– tokenï¼ˆæœ‰ç¼“å­˜å°±ä¸éœ€è¦ç™»å½•ï¼‰
    accounts = app.get_accounts()
    result = None
    if accounts:
        result = app.acquire_token_silent(GRAPH_SCOPES, account=accounts[0])

    # 2) ç¼“å­˜æ²¡æœ‰/è¿‡æœŸ â†’ èµ° device code
    if not result:
        flow = app.initiate_device_flow(scopes=GRAPH_SCOPES)
        if "user_code" not in flow:
            raise RuntimeError(f"Failed to create device flow: {flow}")

        print(flow["message"])  # ä¼šæç¤ºå»å“ªä¸ªç½‘å€è¾“å…¥ code
        result = app.acquire_token_by_device_flow(flow)

    _save_cache(cache)

    if "access_token" not in result:
        raise RuntimeError(f"Could not obtain access token: {result.get('error')} {result.get('error_description')}")
    return result["access_token"]


def send_email_via_graph(subject: str, body_text: str, to_addr: str, body_html: str | None = None) -> None:
    """
    ä½¿ç”¨ Graph API /me/sendMail å‘é€é‚®ä»¶
    """
    token = get_graph_access_token()
    url = "https://graph.microsoft.com/v1.0/me/sendMail"

    content_type = "HTML" if body_html else "Text"
    content = body_html if body_html else body_text

    payload = {
        "message": {
            "subject": subject,
            "body": {"contentType": content_type, "content": content},
            "toRecipients": [{"emailAddress": {"address": to_addr}}],
        },
        "saveToSentItems": True,
    }

    r = requests.post(
        url,
        headers={
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
        },
        data=json.dumps(payload),
        timeout=30,
    )

    if not (200 <= r.status_code < 300):
        raise RuntimeError(f"Graph sendMail failed: {r.status_code} {r.text}")
# -----------------------------
# 3) Notion write
# -----------------------------
def notion_headers() -> Dict[str, str]:
    return {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json",
    }


def notion_rich_text(s: str) -> Dict[str, Any]:
    return {"rich_text": [{"type": "text", "text": {"content": s[:2000]}}]}


def notion_title(s: str) -> Dict[str, Any]:
    return {"title": [{"type": "text", "text": {"content": s[:2000]}}]}


def notion_select(name: str) -> Dict[str, Any]:
    return {"select": {"name": name}}


def notion_url(u: str) -> Dict[str, Any]:
    return {"url": u}


def notion_number(x: Optional[float]) -> Dict[str, Any]:
    return {"number": x}


def notion_date(d) -> Dict[str, Any]:
    """
    Notion date.start éœ€è¦ ISO 8601ï¼Œå¦‚ '2025-12-19'
    å…¼å®¹ pandas NaN / datetime / 'nan' / '2025-12-19 00:00:00' ç­‰
    """
    if d is None:
        return {"date": None}

    # pandas NaN
    try:
        if pd.isna(d):
            return {"date": None}
    except Exception:
        pass

    s = str(d).strip()
    if not s or s.lower() == "nan":
        return {"date": None}

    # åªå–æ—¥æœŸéƒ¨åˆ†ï¼š'YYYY-MM-DD'
    # å…¼å®¹ '2025-12-19 00:00:00' / '2025-12-19T...' / '2025-12-19'
    date_part = s[:10]
    return {"date": {"start": date_part}}


def notion_multi_select(values: list[str]) -> dict:
    # Notion multi_select: [{"name": "xxx"}, ...]
    vals = []
    for v in values or []:
        if not v:
            continue
        vals.append({"name": str(v)[:100]})
    return {"multi_select": vals}
def notion_query_database(filter_obj: Dict[str, Any], page_size: int = 5) -> Dict[str, Any]:
    url = f"https://api.notion.com/v1/databases/{NOTION_DB_ID}/query"
    payload = {"filter": filter_obj, "page_size": page_size}
    r = requests.post(url, headers=notion_headers(), data=json.dumps(payload), timeout=20)
    if not (200 <= r.status_code < 300):
        raise RuntimeError(f"Notion query failed: {r.status_code} {r.text}")
    return r.json()

def notion_page_exists_by_job_url(job_url: str) -> bool:
    if not job_url:
        return False
    filter_obj = {
        "property": NOTION_PROPS["å²—ä½é“¾æ¥"],
        "url": {"equals": job_url}
    }
    data = notion_query_database(filter_obj, page_size=1)
    return len(data.get("results", [])) > 0

def notion_page_exists_by_title_company_location(job_title: str, company: str, location: str) -> bool:
    job_title = (job_title or "").strip()
    company = (company or "").strip()
    location = (location or "").strip()
    if not job_title or not company:
        return False

    filter_obj = {
        "and": [
            {"property": NOTION_PROPS["å²—ä½åç§°"], "title": {"equals": job_title}},
            {"property": NOTION_PROPS["å…¬å¸åç§°"], "rich_text": {"equals": company}},
            {"property": NOTION_PROPS["å·¥ä½œåœ°ç‚¹"], "rich_text": {"equals": location}},
        ]
    }
    data = notion_query_database(filter_obj, page_size=1)
    return len(data.get("results", [])) > 0

def create_notion_page(item: Dict[str, Any], batch_tag: str, platform_default: str = "linkedin") -> None:
    """
    item: assistant è¾“å‡ºçš„æ¯æ¡å²—ä½ JSON
    batch_tag: æœ¬æ¬¡è·‘æ‰¹æ ‡è¯†ï¼Œæ¯”å¦‚ 20251219_1041
    """

    # å…¼å®¹ï¼šassistant è¾“å‡º key å¯èƒ½æ˜¯ job_title/company/locationï¼Œä¹Ÿå¯èƒ½æ˜¯ title/company/location
    job_title = item.get("job_title") or item.get("title") or "æœªçŸ¥å²—ä½"
    company = item.get("company") or item.get("company_name") or ""
    location = item.get("location") or ""
    job_url = item.get("job_url") or ""
    date_posted = item.get("date_posted")  # ä½ åœ¨ main() é‡Œå·²ç» merge è¿‡

    if job_url and notion_page_exists_by_job_url(job_url):
        print(f"[Notion] Skip duplicate (job_url): {job_url}")
        return

    # 2) å†ç”¨ title+company+location å…œåº•ï¼ˆé˜²æ­¢æŸäº›å¹³å° job_url ä¸ç¨³å®šï¼‰
    if notion_page_exists_by_title_company_location(job_title, company, location):
        print(f"[Notion] Skip duplicate (title+company+location): {job_title} | {company} | {location}")
        return

    contract_guess = item.get("contract_type_guess") or "Unknown"
    verdict = item.get("verdict") or "è°¨æ…æŠ•"
    risk = item.get("risk_flag") or "medium"
    score = item.get("score")
    apply_strategy = item.get("apply_strategy") or ""
    overall_advice = item.get("overall_advice") or ""

    match_reasons = item.get("match_reasons") or []
    gaps = item.get("gaps") or []
    keywords = item.get("keywords") or []

    # æ‹›è˜å¹³å°ï¼šä¼˜å…ˆç”¨ item é‡Œçš„ siteï¼Œå¦åˆ™ç”¨é»˜è®¤ linkedin
    site = item.get("site") or platform_default

    props: Dict[str, Any] = {}
    
    # Title
    props[NOTION_PROPS["å²—ä½åç§°"]] = notion_title(job_title)

    # åŸºç¡€ä¿¡æ¯
    props[NOTION_PROPS["å…¬å¸åç§°"]] = notion_rich_text(company)
    props[NOTION_PROPS["æ‹›è˜å¹³å°"]] = notion_select(site)
    props[NOTION_PROPS["å²—ä½é“¾æ¥"]] = notion_url(job_url)
    props[NOTION_PROPS["å·¥ä½œåœ°ç‚¹"]] = notion_rich_text(location)
    props[NOTION_PROPS["å‘å¸ƒæ—¶é—´"]] = notion_date(date_posted)
    props[NOTION_PROPS["åˆåŒç±»å‹ï¼ˆæ¨æ–­ï¼‰"]] = notion_select(contract_guess)

    # å†³ç­–åŒº
    # score å¯èƒ½æ˜¯ int/float/strï¼Œåšä¸€æ¬¡å®‰å…¨è½¬æ¢
    try:
        score_num = float(score) if score is not None else None
    except:
        score_num = None
    props[NOTION_PROPS["åŒ¹é…è¯„åˆ†"]] = notion_number(score_num)
    props[NOTION_PROPS["æ˜¯å¦å€¼å¾—æŠ•é€’"]] = notion_select(verdict)
    props[NOTION_PROPS["é£é™©ç­‰çº§"]] = notion_select(risk)
    props[NOTION_PROPS["æŠ•é€’ç­–ç•¥"]] = notion_rich_text(apply_strategy)

    # åˆ†æè¯´æ˜
    props[NOTION_PROPS["åŒ¹é…åŸå› "]] = notion_rich_text("ï¼›".join([str(x) for x in match_reasons])[:2000])
    props[NOTION_PROPS["ä¸»è¦ç¼ºå£"]] = notion_rich_text("ï¼›".join([str(x) for x in gaps])[:2000])
    props[NOTION_PROPS["å…³é”®è¯"]] = notion_multi_select([str(x) for x in keywords])
    props[NOTION_PROPS["æ€»ä½“å»ºè®®"]] = notion_rich_text(overall_advice[:2000])

    # ç•™æ¡£
    props[NOTION_PROPS["åŸå§‹åˆ†æ JSON"]] = notion_rich_text(json.dumps(item, ensure_ascii=False)[:1900])
    props[NOTION_PROPS["åˆ†ææ—¥æœŸ"]] = {"date": {"start": datetime.now().strftime("%Y-%m-%d")}}
    props[NOTION_PROPS["æ•°æ®æ¥æºæ‰¹æ¬¡"]] = notion_rich_text(batch_tag)

    payload = {"parent": {"database_id": NOTION_DB_ID}, "properties": props}

    r = requests.post(
        "https://api.notion.com/v1/pages",
        headers=notion_headers(),
        data=json.dumps(payload),
        timeout=20,
    )
    if not (200 <= r.status_code < 300):
        raise RuntimeError(f"Notion create page failed: {r.status_code} {r.text}")



# -----------------------------
# 4) Main
# -----------------------------
def main():
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    stats = {
        "jobspy_found": 0,
        "sent_to_ai": 0,
        "ai_received": 0,
        "ai_analyzed": 0,
        "notion_written": 0,
        "batch_mode": AI_CONFIG.get("batch_size", 0) > 0,
        "batch_size": AI_CONFIG.get("batch_size", 0),
        "batches_processed": 0,
        "batches_failed": 0,
        "max_retries": AI_CONFIG.get("max_retries", 3),
        "timestamp": datetime.now().strftime("%Y%m%d_%H%M"),
        "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    # =========================
    # 1ï¸âƒ£ JobSpy æŠ“å–
    # =========================
    jobs_df = fetch_jobs()
    stats["jobspy_found"] = len(jobs_df)
    
    print(f"[JobSpy] æœç´¢åˆ° {stats['jobspy_found']} ä¸ªå²—ä½")
    print(
        f"[JobSpy] ç©ºæè¿°æ•°é‡: "
        f"{(jobs_df['description'].fillna('').str.len() == 0).sum()}/{len(jobs_df)}"
    )

    if jobs_df.empty:
        print("[WARN] æœªæ‰¾åˆ°ä»»ä½•å²—ä½ï¼Œé€€å‡ºç¨‹åº")
        return

    ensure_assistant_has_cv_vector_store(
        client=client,
        assistant_id=ASSISTANT_ID,
        vector_store_id=CV_VECTOR_STORE_ID,
    )

    jobs_payload = jobs_df_to_payload(jobs_df)
    stats["sent_to_ai"] = len(jobs_payload)
    print(f"[å‡†å¤‡] å‘é€ç»™ AI {stats['sent_to_ai']} æ¡å²—ä½æ•°æ®")
    
    # è¯Šæ–­ï¼šæ£€æŸ¥ jobs_payload
    print(f"[DEBUG] jobs_payload å‰ 3 ä¸ªå²—ä½çš„ job_url:")
    for i, job in enumerate(jobs_payload[:3]):
        print(f"  [{i+1}] {job.get('job_url', 'NO URL')}")

    # =========================
    # 2ï¸âƒ£ AI åˆ†æ
    # =========================
    batch_size = AI_CONFIG.get("batch_size", 0)
    
    if batch_size > 0 and len(jobs_payload) > batch_size:
        # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹ç‹¬ç«‹ threadï¼Œé¿å…å¯¹è¯å†å²ç´¯ç§¯ï¼‰
        print(f"[AI] åˆ†æ‰¹å¤„ç†æ¨¡å¼ï¼šæ¯æ‰¹ {batch_size} ä¸ªå²—ä½ï¼Œå…± {len(jobs_payload)} ä¸ª")
        
        results = []
        assistant_raw_texts = []
        failed_batches = []
        
        # ä¿å­˜æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®ï¼ˆè°ƒè¯•ç”¨ï¼‰
        all_batches_file = f"batch_data_{stats['timestamp']}.txt"
        with open(all_batches_file, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write(f"åˆ†æ‰¹å¤„ç†ï¼šå…± {len(jobs_payload)} ä¸ªå²—ä½ï¼Œæ¯æ‰¹ {batch_size} ä¸ª\n")
            f.write("æ¯æ‰¹ä½¿ç”¨ç‹¬ç«‹ threadï¼Œé¿å…å¯¹è¯å†å²ç´¯ç§¯\n")
            f.write("=" * 80 + "\n\n")
        
        total_batches = (len(jobs_payload) + batch_size - 1) // batch_size
        
        # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹åˆ›å»ºæ–° thread
        for i in range(0, len(jobs_payload), batch_size):
            batch = jobs_payload[i:i+batch_size]
            batch_num = i // batch_size + 1
            
            print(f"[AI] å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}ï¼ˆ{len(batch)} ä¸ªå²—ä½ï¼‰")
            
            # ä½¿ç”¨ç²¾ç®€çš„ promptï¼ˆå»æ‰å†—ä½™å†…å®¹ï¼‰
            user_prompt = build_user_prompt(batch)
            
            # ä¿å­˜æœ¬æ‰¹æ¬¡çš„å®Œæ•´æ•°æ®
            with open(all_batches_file, "a", encoding="utf-8") as f:
                f.write(f"\n{'='*80}\n")
                f.write(f"æ‰¹æ¬¡ {batch_num}/{total_batches}ï¼ˆ{len(batch)} ä¸ªå²—ä½ï¼‰\n")
                f.write(f"{'='*80}\n\n")
                f.write(f"ã€å‘é€çš„ Promptã€‘\n")
                f.write(f"é•¿åº¦: {len(user_prompt)} å­—ç¬¦\n")
                f.write("-" * 80 + "\n")
                f.write(user_prompt)
                f.write("\n\n")
            
            try:
                batch_results, batch_raw_text = run_assistant_analysis(
                    client=client,
                    assistant_id=ASSISTANT_ID,
                    user_prompt=user_prompt,
                    expected_jobs=len(batch),
                    max_retries=AI_CONFIG.get("max_retries", 3),
                )
                
                results.extend(batch_results)
                assistant_raw_texts.append(f"\n{'='*60}\næ‰¹æ¬¡ {batch_num}/{total_batches}\n{'='*60}\n{batch_raw_text}")
                
                print(f"[AI] æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œå¾—åˆ° {len(batch_results)} æ¡ç»“æœ")
                
                # ä¿å­˜æœ¬æ‰¹æ¬¡çš„ AI è¿”å›ç»“æœ
                with open(all_batches_file, "a", encoding="utf-8") as f:
                    f.write(f"ã€AI è¿”å›ç»“æœã€‘\n")
                    f.write(f"è¿”å›ç»“æœæ•°: {len(batch_results)}\n")
                    f.write("-" * 80 + "\n")
                    f.write("åŸå§‹è¾“å‡º:\n")
                    f.write(batch_raw_text)
                    f.write("\n\n")
                    f.write("è§£æåçš„ JSON:\n")
                    f.write(json.dumps(batch_results, ensure_ascii=False, indent=2))
                    f.write("\n\n")
                
            except Exception as e:
                print(f"[ERROR] æ‰¹æ¬¡ {batch_num} å¤±è´¥: {e}")
                failed_batches.append({
                    "batch_num": batch_num,
                    "jobs": batch,
                    "error": str(e)
                })
                
                # ä¿å­˜å¤±è´¥ä¿¡æ¯
                with open(all_batches_file, "a", encoding="utf-8") as f:
                    f.write(f"ã€æ‰¹æ¬¡å¤±è´¥ã€‘\n")
                    f.write(f"é”™è¯¯: {str(e)}\n\n")
                
                # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹
            
            # æ‰¹æ¬¡ä¹‹é—´ç¨å¾®ç­‰å¾…ï¼Œé¿å…è¿ç»­è§¦å‘ rate limit
            if i + batch_size < len(jobs_payload):
                wait_time = 3 if batch_num < 5 else 5  # åé¢çš„æ‰¹æ¬¡ç­‰å¾…æ›´é•¿æ—¶é—´
                print(f"[AI] ç­‰å¾… {wait_time} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹...")
                time.sleep(wait_time)
        
        assistant_raw_text = "\n\n".join(assistant_raw_texts)
        
        # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æ±‡æ€»ä¿¡æ¯
        with open(all_batches_file, "a", encoding="utf-8") as f:
            f.write("\n" + "=" * 80 + "\n")
            f.write("ã€æ‰€æœ‰æ‰¹æ¬¡æ±‡æ€»ã€‘\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"æ€»æ‰¹æ¬¡æ•°: {total_batches}\n")
            f.write(f"æˆåŠŸæ‰¹æ¬¡: {total_batches - len(failed_batches)}\n")
            f.write(f"å¤±è´¥æ‰¹æ¬¡: {len(failed_batches)}\n")
            f.write(f"æ€»åˆ†æç»“æœ: {len(results)} ä¸ªå²—ä½\n\n")
            
            if len(results) > 0:
                f.write("ã€æ‰€æœ‰å²—ä½æ±‡æ€»ã€‘\n")
                f.write("-" * 80 + "\n")
                f.write(json.dumps(results, ensure_ascii=False, indent=2))
                f.write("\n\n")
            
            f.write("=" * 80 + "\n")
            f.write("æ–‡ä»¶ç»“æŸ\n")
            f.write("=" * 80 + "\n")
        
        print(f"[DEBUG] æ‰€æœ‰æ‰¹æ¬¡æ•°æ®å·²ä¿å­˜åˆ°: {all_batches_file}")
        print(f"[DEBUG] æ–‡ä»¶åŒ…å«: å®Œæ•´ Prompt + AI è¿”å› + æ±‡æ€»ï¼Œå…± {len(results)} ä¸ªåˆ†æç»“æœ")
        
        # ç»Ÿè®¡æ¯æ‰¹çš„å®Œæˆæƒ…å†µ
        total_batches = (len(jobs_payload) + batch_size - 1) // batch_size
        successful_batches = total_batches - len(failed_batches)
        print(f"[AI] åˆ†æ‰¹å¤„ç†å®Œæˆï¼šå¤„ç†äº† {total_batches} æ‰¹ï¼ŒæˆåŠŸ {successful_batches} æ‰¹ï¼Œå¤±è´¥ {len(failed_batches)} æ‰¹")
        print(f"[AI] å…±å¾—åˆ° {len(results)} æ¡æˆåŠŸåˆ†æç»“æœ")
        
        stats["batches_processed"] = total_batches
        stats["batches_failed"] = len(failed_batches)
        
        # å¦‚æœæœ‰å¤±è´¥çš„æ‰¹æ¬¡ï¼Œè®°å½•ä¸‹æ¥
        if failed_batches:
            failed_file = f"failed_batches_{stats['timestamp']}.json"
            with open(failed_file, "w", encoding="utf-8") as f:
                json.dump(failed_batches, f, ensure_ascii=False, indent=2)
            print(f"[WARN] å¤±è´¥æ‰¹æ¬¡è¯¦æƒ…å·²ä¿å­˜åˆ°: {failed_file}")
            print(f"[WARN] å¤±è´¥çš„æ‰¹æ¬¡ç¼–å·: {[b['batch_num'] for b in failed_batches]}")
        
    else:
        # ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å²—ä½
        print(f"[AI] ä¸€æ¬¡æ€§å¤„ç† {len(jobs_payload)} ä¸ªå²—ä½")
        
        user_prompt = build_user_prompt(jobs_payload)
        
        # è¯Šæ–­ï¼šä¿å­˜å‘é€ç»™ AI çš„ prompt å’Œ payload
        prompt_debug_file = f"prompt_sent_to_ai_{stats['timestamp']}.txt"
        with open(prompt_debug_file, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write("å‘é€ç»™ AI çš„å®Œæ•´ Prompt\n")
            f.write("=" * 80 + "\n\n")
            f.write(user_prompt)
            f.write("\n\n" + "=" * 80 + "\n")
            f.write(f"jobs_payload åŒ…å« {len(jobs_payload)} ä¸ªå²—ä½\n")
            f.write("=" * 80 + "\n\n")
            f.write(json.dumps(jobs_payload, ensure_ascii=False, indent=2))
        print(f"[DEBUG] Prompt å·²ä¿å­˜åˆ°: {prompt_debug_file}")
        print(f"[DEBUG] Prompt é•¿åº¦: {len(user_prompt)} å­—ç¬¦")
        print(f"[DEBUG] Prompt ä¸­åŒ…å«çš„å²—ä½æ•°æ®: {len(jobs_payload)} æ¡")
        
        results, assistant_raw_text = run_assistant_analysis(
            client=client,
            assistant_id=ASSISTANT_ID,
            user_prompt=user_prompt,
            expected_jobs=len(jobs_payload),
            max_retries=AI_CONFIG.get("max_retries", 3),
        )
        
        stats["batches_processed"] = 1

    stats["ai_received"] = len(results)
    stats["ai_analyzed"] = len([r for r in results if isinstance(r, dict) and r.get("job_url")])
    
    print(f"[AI] AI è¿”å›äº† {stats['ai_received']} æ¡åˆ†æç»“æœ")
    print(f"[AI] æˆåŠŸåˆ†æ {stats['ai_analyzed']} ä¸ªå²—ä½")
    
    # è¯Šæ–­ï¼šæ£€æŸ¥è¿”å›çš„ç»“æœ
    if stats['ai_received'] != stats['sent_to_ai']:
        print(f"[WARN] [!] AI è¿”å›æ•°é‡ä¸åŒ¹é…ï¼å‘é€äº† {stats['sent_to_ai']} æ¡ï¼Œä½†åªæ”¶åˆ° {stats['ai_received']} æ¡")
        print(f"[WARN] è¿™å¯èƒ½æ˜¯å› ä¸ºï¼š")
        print(f"  1. AI è¾“å‡º token é™åˆ¶ï¼šAssistant çš„ max_tokens è®¾ç½®å¤ªå°")
        print(f"  2. è¾“å…¥å†…å®¹å¤ªé•¿ï¼šAI æ— æ³•åœ¨å•æ¬¡è°ƒç”¨ä¸­å¤„ç†æ‰€æœ‰å²—ä½")
        print(f"  3. AI ç†è§£é”™è¯¯ï¼šAI å¯èƒ½åªåˆ†æäº†ç¬¬ä¸€ä¸ªå²—ä½")
        print(f"[å¼ºçƒˆå»ºè®®] å¯ç”¨åˆ†æ‰¹å¤„ç†æ¨¡å¼ï¼šåœ¨ä»£ç ä¸­è®¾ç½® AI_CONFIG['batch_size'] = 3")
        if 'prompt_debug_file' in locals():
            print(f"[è°ƒè¯•] è¯·æŸ¥çœ‹ {prompt_debug_file} äº†è§£å‘é€ç»™ AI çš„å®Œæ•´å†…å®¹")
    
    print(f"[DEBUG] AI è¿”å›çš„ job_url åˆ—è¡¨ï¼ˆå‰ 5 ä¸ªï¼‰:")
    for i, r in enumerate(results[:5]):
        if isinstance(r, dict):
            print(f"  [{i+1}] {r.get('job_url', 'NO URL')}")

    # =========================
    # 3ï¸âƒ£ å¯¹é½ JobSpy â†” AI
    # =========================
    jobs_by_url = {j["job_url"]: j for j in jobs_payload if j.get("job_url")}
    jobs_urls = list(jobs_by_url.keys())
    res_urls = [r.get("job_url") for r in results if isinstance(r, dict)]

    print("[DEBUG] first 3 job urls:", jobs_urls[:3])
    print("[DEBUG] result urls:", res_urls)

    missing = [u for u in jobs_urls if u not in set(res_urls)]
    print("[DEBUG] missing urls:", missing)

    # merge å› date_posted / site
    for item in results:
        j = jobs_by_url.get(item.get("job_url"), {})
        item["date_posted"] = j.get("date_posted")
        item["site"] = j.get("site")

    # =========================
    # 4ï¸âƒ£ å†™å…¥ Notion
    # =========================
    batch_tag = stats["timestamp"]
    notion_success = 0
    for item in results:
        try:
            create_notion_page(
                item,
                batch_tag=batch_tag,
                platform_default="linkedin"
            )
            notion_success += 1
        except Exception as e:
            print(f"[Notion] å†™å…¥å¤±è´¥: {item.get('job_url', 'unknown')}, é”™è¯¯: {e}")

    stats["notion_written"] = notion_success
    print(f"[Notion] æˆåŠŸå†™å…¥ {stats['notion_written']}/{len(results)} æ¡è®°å½•åˆ° Notion")

    # =========================
    # 5ï¸âƒ£ ç”Ÿæˆå¹¶å‘é€"ä»Šæ—¥å²—ä½å°æŠ¥å‘Š"
    # =========================
    print("\n" + "=" * 60)
    print("ã€ç”Ÿæˆæ¯æ—¥æŠ¥å‘Šã€‘")
    print("=" * 60)
    
    try:
        report_text = run_daily_report_text(client, ASSISTANT_ID, results)
        print("[æŠ¥å‘Š] [OK] æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
    except Exception as e:
        print(f"[æŠ¥å‘Š] [WARN] AI æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        print("[æŠ¥å‘Š] ä½¿ç”¨å¤‡ç”¨ç®€åŒ–æŠ¥å‘Š...")
        today = datetime.now().strftime("%Y-%m-%d")
        report_text = generate_fallback_report(results, today)
        print("[æŠ¥å‘Š] [OK] å¤‡ç”¨æŠ¥å‘Šç”ŸæˆæˆåŠŸ")

    # =========================
    # 6ï¸âƒ£ æ•´åˆæ‰€æœ‰è¾“å‡ºåˆ°ä¸€ä¸ªæ–‡ä»¶
    # =========================
    consolidated_output = f"consolidated_report_{stats['timestamp']}.txt"
    
    with open(consolidated_output, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("JobSpy è‡ªåŠ¨åŒ–å²—ä½åˆ†æ - ç»¼åˆæŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        f.write("ã€æ‰§è¡Œç»Ÿè®¡ã€‘\n")
        f.write(f"æ‰§è¡Œæ—¶é—´: {stats['date']}\n")
        f.write(f"æ‰¹æ¬¡æ ‡è¯†: {stats['timestamp']}\n")
        f.write(f"JobSpy æœç´¢åˆ°å²—ä½æ•°: {stats['jobspy_found']}\n")
        f.write(f"å‘é€ç»™ AI çš„å²—ä½æ•°: {stats['sent_to_ai']}\n")
        f.write(f"AI å¤„ç†æ¨¡å¼: {'åˆ†æ‰¹å¤„ç†' if stats['batch_mode'] else 'ä¸€æ¬¡æ€§å¤„ç†'}\n")
        if stats['batch_mode']:
            f.write(f"  - æ¯æ‰¹å¤§å°: {stats['batch_size']} ä¸ªå²—ä½\n")
            f.write(f"  - å¤„ç†æ‰¹æ¬¡æ•°: {stats.get('batches_processed', 0)}\n")
            if stats.get('batches_failed', 0) > 0:
                f.write(f"  - å¤±è´¥æ‰¹æ¬¡æ•°: {stats['batches_failed']}\n")
        f.write(f"  - æœ€å¤§é‡è¯•æ¬¡æ•°: {stats['max_retries']}\n")
        f.write(f"AI è¿”å›ç»“æœæ•°: {stats['ai_received']}\n")
        f.write(f"AI æˆåŠŸåˆ†æå²—ä½æ•°: {stats['ai_analyzed']}\n")
        f.write(f"Notion æˆåŠŸå†™å…¥æ•°: {stats['notion_written']}\n")
        f.write("\n" + "=" * 80 + "\n\n")
        
        # æ¯æ—¥æŠ¥å‘Š
        f.write("ã€ä»Šæ—¥å²—ä½å°æŠ¥å‘Šã€‘\n")
        f.write("-" * 80 + "\n")
        f.write(report_text)
        f.write("\n\n" + "=" * 80 + "\n\n")
        
        # è¯¦ç»†åˆ†æç»“æœ
        f.write("ã€è¯¦ç»†åˆ†æç»“æœ JSONã€‘\n")
        f.write("-" * 80 + "\n")
        f.write(json.dumps(results, ensure_ascii=False, indent=2))
        f.write("\n\n" + "=" * 80 + "\n\n")
        
        # AI åŸå§‹è¾“å‡ºï¼ˆä¾›è°ƒè¯•ï¼‰
        f.write("ã€AI åŸå§‹è¾“å‡ºï¼ˆå®Œæ•´ï¼‰ã€‘\n")
        f.write("-" * 80 + "\n")
        f.write(f"è¾“å‡ºé•¿åº¦: {len(assistant_raw_text)} å­—ç¬¦\n")
        f.write("-" * 80 + "\n")
        f.write(assistant_raw_text)  # ä¿å­˜å®Œæ•´è¾“å‡ºç”¨äºè°ƒè¯•
        f.write("\n\n" + "=" * 80 + "\n")
        f.write("æŠ¥å‘Šç»“æŸ\n")
        f.write("=" * 80 + "\n")

    print(f"\n{'='*60}")
    print(f"ã€æ‰§è¡Œå®Œæˆ - ç»Ÿè®¡æ‘˜è¦ã€‘")
    print(f"{'='*60}")
    print(f"JobSpy æœç´¢åˆ°:     {stats['jobspy_found']} ä¸ªå²—ä½")
    print(f"å‘é€ç»™ AI:         {stats['sent_to_ai']} æ¡")
    if stats['batch_mode']:
        success_rate = ((stats.get('batches_processed', 0) - stats.get('batches_failed', 0)) / 
                       stats.get('batches_processed', 1) * 100) if stats.get('batches_processed', 0) > 0 else 0
        print(f"å¤„ç†æ¨¡å¼:          åˆ†æ‰¹ï¼ˆæ¯æ‰¹ {stats['batch_size']} ä¸ªï¼Œ"
              f"å…± {stats.get('batches_processed', 0)} æ‰¹ï¼Œ"
              f"æˆåŠŸç‡ {success_rate:.1f}%ï¼‰")
        if stats.get('batches_failed', 0) > 0:
            print(f"                   [!] {stats['batches_failed']} æ‰¹å¤±è´¥")
    else:
        print(f"å¤„ç†æ¨¡å¼:          ä¸€æ¬¡æ€§")
    print(f"AI è¿”å›:           {stats['ai_received']} æ¡")
    print(f"AI æˆåŠŸåˆ†æ:       {stats['ai_analyzed']} æ¡")
    print(f"Notion å†™å…¥æˆåŠŸ:   {stats['notion_written']} æ¡")
    print(f"{'='*60}")
    print(f"ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {consolidated_output}")
    print(f"{'='*60}\n")

    # å‘é‚®ä»¶ï¼ˆMicrosoft Graphï¼‰
    print("\n" + "=" * 60)
    print("ã€å‘é€é‚®ä»¶ã€‘")
    print("=" * 60)
    
    to_addr = os.getenv("EMAIL_TO")
    if not to_addr:
        print("[Email] [WARN] æœªè®¾ç½® EMAIL_TO ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡é‚®ä»¶å‘é€")
    else:
        try:
            print(f"[Email] æ”¶ä»¶äºº: {to_addr}")
            subject = f"ğŸ“Œ ä»Šæ—¥å²—ä½å°æŠ¥å‘Šï½œ{datetime.now():%Y-%m-%d}ï½œå…±{len(results)}æ¡"
            print(f"[Email] ä¸»é¢˜: {subject}")
            print(f"[Email] æŠ¥å‘Šé•¿åº¦: {len(report_text)} å­—ç¬¦")
            
            report_html = text_to_simple_html(report_text)
            print(f"[Email] HTML é•¿åº¦: {len(report_html)} å­—ç¬¦")
            
            send_email_via_graph(subject=subject, body_text=report_text, body_html=report_html, to_addr=to_addr)
            print("[Email] [OK] é‚®ä»¶å·²é€šè¿‡ Microsoft Graph æˆåŠŸå‘é€ï¼")
        except Exception as e:
            print(f"[Email] [WARN] é‚®ä»¶å‘é€å¤±è´¥: {e}")
            print(f"[Email] æŠ¥å‘Šå·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: {consolidated_output}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ç¨‹åºç»§ç»­å®Œæˆ
    
    print("=" * 60)
    print("[OK] æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("=" * 60)

def safe_main():
    try:
        main()
    except Exception as e:
        with open("run_error.log", "a", encoding="utf-8") as f:
            f.write(f"\n[{datetime.now()}] ERROR\n")
            f.write(traceback.format_exc())
        raise

if __name__ == "__main__":
    safe_main()

